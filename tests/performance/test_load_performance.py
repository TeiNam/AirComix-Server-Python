#!/usr/bin/env python3
"""
Comix Server ì„±ëŠ¥ ë° ë¡œë“œ í…ŒìŠ¤íŠ¸

ì„œë²„ì˜ ì„±ëŠ¥ íŠ¹ì„±ê³¼ ë¶€í•˜ ì²˜ë¦¬ ëŠ¥ë ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import asyncio
import concurrent.futures
import json
import statistics
import time
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import threading
import psutil
import requests
from dataclasses import dataclass, asdict


@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤"""
    response_time: float
    status_code: int
    content_length: int
    error: Optional[str] = None
    timestamp: float = 0.0
    
    def __post_init__(self):
        if self.timestamp == 0.0:
            self.timestamp = time.time()


@dataclass
class LoadTestResult:
    """ë¡œë“œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    total_requests: int
    successful_requests: int
    failed_requests: int
    average_response_time: float
    min_response_time: float
    max_response_time: float
    p95_response_time: float
    p99_response_time: float
    requests_per_second: float
    total_duration: float
    error_rate: float
    throughput_mb_per_sec: float


class PerformanceTester:
    """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self, base_url: str = "http://localhost:31257"):
        self.base_url = base_url.rstrip('/')
        self.session = requests.Session()
        
    def single_request_test(self, endpoint: str, timeout: float = 10.0) -> PerformanceMetrics:
        """ë‹¨ì¼ ìš”ì²­ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        start_time = time.time()
        
        try:
            response = self.session.get(
                f"{self.base_url}{endpoint}",
                timeout=timeout
            )
            
            response_time = time.time() - start_time
            content_length = len(response.content) if response.content else 0
            
            return PerformanceMetrics(
                response_time=response_time,
                status_code=response.status_code,
                content_length=content_length,
                timestamp=start_time
            )
            
        except Exception as e:
            response_time = time.time() - start_time
            return PerformanceMetrics(
                response_time=response_time,
                status_code=0,
                content_length=0,
                error=str(e),
                timestamp=start_time
            )
    
    def concurrent_load_test(
        self,
        endpoint: str,
        num_requests: int,
        concurrency: int,
        timeout: float = 10.0
    ) -> LoadTestResult:
        """ë™ì‹œ ìš”ì²­ ë¡œë“œ í…ŒìŠ¤íŠ¸"""
        
        def make_request() -> PerformanceMetrics:
            return self.single_request_test(endpoint, timeout)
        
        start_time = time.time()
        metrics: List[PerformanceMetrics] = []
        
        # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•œ ë™ì‹œ ìš”ì²­
        with concurrent.futures.ThreadPoolExecutor(max_workers=concurrency) as executor:
            futures = [executor.submit(make_request) for _ in range(num_requests)]
            
            for future in concurrent.futures.as_completed(futures):
                try:
                    metric = future.result()
                    metrics.append(metric)
                except Exception as e:
                    metrics.append(PerformanceMetrics(
                        response_time=0.0,
                        status_code=0,
                        content_length=0,
                        error=str(e)
                    ))
        
        total_duration = time.time() - start_time
        
        return self._calculate_load_test_result(metrics, total_duration)
    
    def sustained_load_test(
        self,
        endpoint: str,
        duration_seconds: int,
        requests_per_second: int
    ) -> LoadTestResult:
        """ì§€ì†ì ì¸ ë¶€í•˜ í…ŒìŠ¤íŠ¸"""
        
        metrics: List[PerformanceMetrics] = []
        start_time = time.time()
        end_time = start_time + duration_seconds
        
        request_interval = 1.0 / requests_per_second
        
        def make_requests():
            last_request_time = time.time()
            
            while time.time() < end_time:
                current_time = time.time()
                
                # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
                if current_time - last_request_time >= request_interval:
                    metric = self.single_request_test(endpoint, timeout=5.0)
                    metrics.append(metric)
                    last_request_time = current_time
                else:
                    time.sleep(0.001)  # 1ms ëŒ€ê¸°
        
        # ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ìš”ì²­ ì‹¤í–‰
        thread = threading.Thread(target=make_requests)
        thread.start()
        thread.join()
        
        actual_duration = time.time() - start_time
        
        return self._calculate_load_test_result(metrics, actual_duration)
    
    def memory_stress_test(
        self,
        large_image_endpoint: str,
        num_concurrent_downloads: int,
        timeout: float = 30.0
    ) -> Dict[str, any]:
        """ë©”ëª¨ë¦¬ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ (í° ì´ë¯¸ì§€ ë™ì‹œ ë‹¤ìš´ë¡œë“œ)"""
        
        def download_large_image() -> Tuple[PerformanceMetrics, int]:
            """í° ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •"""
            process = psutil.Process()
            initial_memory = process.memory_info().rss
            
            metric = self.single_request_test(large_image_endpoint, timeout)
            
            final_memory = process.memory_info().rss
            memory_used = final_memory - initial_memory
            
            return metric, memory_used
        
        start_time = time.time()
        results = []
        memory_usage = []
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=num_concurrent_downloads) as executor:
            futures = [executor.submit(download_large_image) for _ in range(num_concurrent_downloads)]
            
            for future in concurrent.futures.as_completed(futures):
                try:
                    metric, memory_used = future.result()
                    results.append(metric)
                    memory_usage.append(memory_used)
                except Exception as e:
                    results.append(PerformanceMetrics(
                        response_time=0.0,
                        status_code=0,
                        content_length=0,
                        error=str(e)
                    ))
                    memory_usage.append(0)
        
        total_duration = time.time() - start_time
        
        return {
            'load_test_result': self._calculate_load_test_result(results, total_duration),
            'memory_metrics': {
                'total_memory_used': sum(memory_usage),
                'average_memory_per_request': statistics.mean(memory_usage) if memory_usage else 0,
                'max_memory_per_request': max(memory_usage) if memory_usage else 0,
                'memory_efficiency': sum(r.content_length for r in results) / max(sum(memory_usage), 1)
            }
        }
    
    def _calculate_load_test_result(
        self,
        metrics: List[PerformanceMetrics],
        total_duration: float
    ) -> LoadTestResult:
        """ë¡œë“œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê³„ì‚°"""
        
        if not metrics:
            return LoadTestResult(
                total_requests=0,
                successful_requests=0,
                failed_requests=0,
                average_response_time=0.0,
                min_response_time=0.0,
                max_response_time=0.0,
                p95_response_time=0.0,
                p99_response_time=0.0,
                requests_per_second=0.0,
                total_duration=total_duration,
                error_rate=1.0,
                throughput_mb_per_sec=0.0
            )
        
        successful_metrics = [m for m in metrics if m.error is None and m.status_code == 200]
        failed_metrics = [m for m in metrics if m.error is not None or m.status_code != 200]
        
        response_times = [m.response_time for m in successful_metrics]
        content_lengths = [m.content_length for m in successful_metrics]
        
        if response_times:
            avg_response_time = statistics.mean(response_times)
            min_response_time = min(response_times)
            max_response_time = max(response_times)
            p95_response_time = statistics.quantiles(response_times, n=20)[18] if len(response_times) > 1 else avg_response_time
            p99_response_time = statistics.quantiles(response_times, n=100)[98] if len(response_times) > 1 else avg_response_time
        else:
            avg_response_time = min_response_time = max_response_time = p95_response_time = p99_response_time = 0.0
        
        total_bytes = sum(content_lengths)
        throughput_mb_per_sec = (total_bytes / (1024 * 1024)) / max(total_duration, 0.001)
        
        return LoadTestResult(
            total_requests=len(metrics),
            successful_requests=len(successful_metrics),
            failed_requests=len(failed_metrics),
            average_response_time=avg_response_time,
            min_response_time=min_response_time,
            max_response_time=max_response_time,
            p95_response_time=p95_response_time,
            p99_response_time=p99_response_time,
            requests_per_second=len(metrics) / max(total_duration, 0.001),
            total_duration=total_duration,
            error_rate=len(failed_metrics) / len(metrics),
            throughput_mb_per_sec=throughput_mb_per_sec
        )


class SystemResourceMonitor:
    """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.monitoring = False
        self.metrics = []
        
    def start_monitoring(self, interval: float = 1.0):
        """ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self.monitoring = True
        self.metrics = []
        
        def monitor():
            while self.monitoring:
                try:
                    cpu_percent = psutil.cpu_percent(interval=None)
                    memory = psutil.virtual_memory()
                    disk_io = psutil.disk_io_counters()
                    network_io = psutil.net_io_counters()
                    
                    self.metrics.append({
                        'timestamp': time.time(),
                        'cpu_percent': cpu_percent,
                        'memory_percent': memory.percent,
                        'memory_used_mb': memory.used / (1024 * 1024),
                        'memory_available_mb': memory.available / (1024 * 1024),
                        'disk_read_mb': disk_io.read_bytes / (1024 * 1024) if disk_io else 0,
                        'disk_write_mb': disk_io.write_bytes / (1024 * 1024) if disk_io else 0,
                        'network_sent_mb': network_io.bytes_sent / (1024 * 1024) if network_io else 0,
                        'network_recv_mb': network_io.bytes_recv / (1024 * 1024) if network_io else 0,
                    })
                    
                    time.sleep(interval)
                except Exception:
                    break
        
        self.monitor_thread = threading.Thread(target=monitor, daemon=True)
        self.monitor_thread.start()
    
    def stop_monitoring(self) -> Dict[str, any]:
        """ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€ ë° ê²°ê³¼ ë°˜í™˜"""
        self.monitoring = False
        
        if not self.metrics:
            return {}
        
        # í†µê³„ ê³„ì‚°
        cpu_values = [m['cpu_percent'] for m in self.metrics]
        memory_values = [m['memory_percent'] for m in self.metrics]
        
        return {
            'duration': self.metrics[-1]['timestamp'] - self.metrics[0]['timestamp'],
            'sample_count': len(self.metrics),
            'cpu': {
                'average': statistics.mean(cpu_values),
                'max': max(cpu_values),
                'min': min(cpu_values)
            },
            'memory': {
                'average_percent': statistics.mean(memory_values),
                'max_percent': max(memory_values),
                'peak_used_mb': max(m['memory_used_mb'] for m in self.metrics)
            },
            'raw_metrics': self.metrics
        }


def run_performance_test_suite():
    """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰"""
    print("âš¡ Comix Server ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    tester = PerformanceTester()
    monitor = SystemResourceMonitor()
    
    # 1. ê¸°ë³¸ ì‘ë‹µ ì‹œê°„ í…ŒìŠ¤íŠ¸
    print("\nğŸ“Š ê¸°ë³¸ ì‘ë‹µ ì‹œê°„ í…ŒìŠ¤íŠ¸")
    print("-" * 30)
    
    endpoints = [
        "/",
        "/health",
        "/welcome.102",
        "/manga/"
    ]
    
    for endpoint in endpoints:
        metric = tester.single_request_test(endpoint)
        if metric.error:
            print(f"âŒ {endpoint}: ì˜¤ë¥˜ - {metric.error}")
        else:
            print(f"âœ… {endpoint}: {metric.response_time:.3f}ì´ˆ (ìƒíƒœ: {metric.status_code})")
    
    # 2. ë™ì‹œ ìš”ì²­ í…ŒìŠ¤íŠ¸
    print("\nğŸ”„ ë™ì‹œ ìš”ì²­ í…ŒìŠ¤íŠ¸")
    print("-" * 30)
    
    monitor.start_monitoring()
    
    concurrent_tests = [
        (10, 5),   # 10ê°œ ìš”ì²­, 5ê°œ ë™ì‹œ
        (50, 10),  # 50ê°œ ìš”ì²­, 10ê°œ ë™ì‹œ
        (100, 20), # 100ê°œ ìš”ì²­, 20ê°œ ë™ì‹œ
    ]
    
    for num_requests, concurrency in concurrent_tests:
        print(f"\nğŸ“ˆ {num_requests}ê°œ ìš”ì²­, {concurrency}ê°œ ë™ì‹œ ì—°ê²°")
        
        result = tester.concurrent_load_test("/", num_requests, concurrency)
        
        print(f"   ì„±ê³µë¥ : {(1 - result.error_rate):.1%}")
        print(f"   í‰ê·  ì‘ë‹µì‹œê°„: {result.average_response_time:.3f}ì´ˆ")
        print(f"   95% ì‘ë‹µì‹œê°„: {result.p95_response_time:.3f}ì´ˆ")
        print(f"   ì²˜ë¦¬ëŸ‰: {result.requests_per_second:.1f} req/sec")
        
        if result.error_rate > 0.1:  # 10% ì´ìƒ ì‹¤íŒ¨ìœ¨
            print(f"   âš ï¸  ë†’ì€ ì‹¤íŒ¨ìœ¨: {result.error_rate:.1%}")
    
    # 3. ì§€ì†ì ì¸ ë¶€í•˜ í…ŒìŠ¤íŠ¸
    print(f"\nâ±ï¸  ì§€ì†ì ì¸ ë¶€í•˜ í…ŒìŠ¤íŠ¸ (30ì´ˆ)")
    print("-" * 30)
    
    sustained_result = tester.sustained_load_test("/", 30, 10)  # 30ì´ˆê°„ ì´ˆë‹¹ 10ìš”ì²­
    
    print(f"   ì´ ìš”ì²­: {sustained_result.total_requests}")
    print(f"   ì„±ê³µë¥ : {(1 - sustained_result.error_rate):.1%}")
    print(f"   í‰ê·  ì‘ë‹µì‹œê°„: {sustained_result.average_response_time:.3f}ì´ˆ")
    print(f"   ì‹¤ì œ ì²˜ë¦¬ëŸ‰: {sustained_result.requests_per_second:.1f} req/sec")
    
    # 4. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
    resource_stats = monitor.stop_monitoring()
    
    if resource_stats:
        print(f"\nğŸ’» ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰")
        print("-" * 30)
        print(f"   í‰ê·  CPU: {resource_stats['cpu']['average']:.1f}%")
        print(f"   ìµœëŒ€ CPU: {resource_stats['cpu']['max']:.1f}%")
        print(f"   í‰ê·  ë©”ëª¨ë¦¬: {resource_stats['memory']['average_percent']:.1f}%")
        print(f"   ìµœëŒ€ ë©”ëª¨ë¦¬: {resource_stats['memory']['peak_used_mb']:.1f}MB")
    
    # 5. ì„±ëŠ¥ ê¸°ì¤€ í‰ê°€
    print(f"\nğŸ¯ ì„±ëŠ¥ ê¸°ì¤€ í‰ê°€")
    print("-" * 30)
    
    # ê¸°ì¤€ê°’ ì„¤ì •
    criteria = {
        'max_response_time': 1.0,      # ìµœëŒ€ 1ì´ˆ
        'min_success_rate': 0.95,      # ìµœì†Œ 95% ì„±ê³µë¥ 
        'min_throughput': 50,          # ìµœì†Œ 50 req/sec
        'max_cpu_usage': 80,           # ìµœëŒ€ 80% CPU
        'max_memory_usage': 70         # ìµœëŒ€ 70% ë©”ëª¨ë¦¬
    }
    
    passed_tests = 0
    total_tests = 0
    
    # ì‘ë‹µ ì‹œê°„ í‰ê°€
    total_tests += 1
    if sustained_result.p95_response_time <= criteria['max_response_time']:
        print(f"âœ… ì‘ë‹µ ì‹œê°„: {sustained_result.p95_response_time:.3f}ì´ˆ â‰¤ {criteria['max_response_time']}ì´ˆ")
        passed_tests += 1
    else:
        print(f"âŒ ì‘ë‹µ ì‹œê°„: {sustained_result.p95_response_time:.3f}ì´ˆ > {criteria['max_response_time']}ì´ˆ")
    
    # ì„±ê³µë¥  í‰ê°€
    total_tests += 1
    success_rate = 1 - sustained_result.error_rate
    if success_rate >= criteria['min_success_rate']:
        print(f"âœ… ì„±ê³µë¥ : {success_rate:.1%} â‰¥ {criteria['min_success_rate']:.1%}")
        passed_tests += 1
    else:
        print(f"âŒ ì„±ê³µë¥ : {success_rate:.1%} < {criteria['min_success_rate']:.1%}")
    
    # ì²˜ë¦¬ëŸ‰ í‰ê°€
    total_tests += 1
    if sustained_result.requests_per_second >= criteria['min_throughput']:
        print(f"âœ… ì²˜ë¦¬ëŸ‰: {sustained_result.requests_per_second:.1f} req/sec â‰¥ {criteria['min_throughput']} req/sec")
        passed_tests += 1
    else:
        print(f"âŒ ì²˜ë¦¬ëŸ‰: {sustained_result.requests_per_second:.1f} req/sec < {criteria['min_throughput']} req/sec")
    
    # ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ í‰ê°€
    if resource_stats:
        total_tests += 2
        
        if resource_stats['cpu']['average'] <= criteria['max_cpu_usage']:
            print(f"âœ… CPU ì‚¬ìš©ëŸ‰: {resource_stats['cpu']['average']:.1f}% â‰¤ {criteria['max_cpu_usage']}%")
            passed_tests += 1
        else:
            print(f"âŒ CPU ì‚¬ìš©ëŸ‰: {resource_stats['cpu']['average']:.1f}% > {criteria['max_cpu_usage']}%")
        
        if resource_stats['memory']['average_percent'] <= criteria['max_memory_usage']:
            print(f"âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {resource_stats['memory']['average_percent']:.1f}% â‰¤ {criteria['max_memory_usage']}%")
            passed_tests += 1
        else:
            print(f"âŒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {resource_stats['memory']['average_percent']:.1f}% > {criteria['max_memory_usage']}%")
    
    # ìµœì¢… ê²°ê³¼
    print(f"\nğŸ† ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼")
    print("=" * 60)
    print(f"í†µê³¼í•œ í…ŒìŠ¤íŠ¸: {passed_tests}/{total_tests}")
    
    if passed_tests == total_tests:
        print("ğŸ‰ ëª¨ë“  ì„±ëŠ¥ ê¸°ì¤€ì„ ë§Œì¡±í•©ë‹ˆë‹¤!")
        return True
    else:
        print("âš ï¸  ì¼ë¶€ ì„±ëŠ¥ ê¸°ì¤€ì„ ë§Œì¡±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        print("ì„±ëŠ¥ ìµœì í™”ë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”.")
        return False


if __name__ == "__main__":
    try:
        success = run_performance_test_suite()
        exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\n\ní…ŒìŠ¤íŠ¸ê°€ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        exit(1)
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        exit(1)